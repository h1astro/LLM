## 1.1 什么是语言模型
语言模型（Language Model）的经典定义是一种对令牌序列（token）的概率分布。
假设我们有一个令牌集的词汇表V。语言模型p为每个令牌序列$x_1,...,x_L \in V$ 分配一个概率(介于0和1之间的数组)：
$p(x_1,...,x_L)$
概率直观地告诉我们一个标记序列有多”好（good）“。

从数学上讲，语言模型是一个非常简单而又美妙的对象。但是这种简单是具有欺骗性的：赋予所有序列以（有意义的）概率的能力，
该能力要求语言模型具有非凡的（但是隐含的）语言能力和世界知识。

例如，语言模型应该隐含地赋予”mouse the the cheese ate“一个非常低的概率，因为它在语法上是不正确的（句法知识）。
由于世界知识的存在，语言模型应该隐含地赋予”the mouse ate the cheese“比"the cheese ate the mouse"更高的概率。
这是因为两个句子在句法上是相同的，但在语义上却存在差异，而语言模型需要具备卓越的语言能力和世界知识，才能准确评估序列的概率。
语言模型也可以做生成任务。如定义所示，语言模型p接受一个序列并返回一个概率来评估其好坏。
我们也可以根据语言模型生成一个序列。最纯粹的方法是从语言模型p中以概率$p(x_{1:L})$进行采样，表示为：
$$
x_{1:L} ~ p.
$$
如何在计算上高效地实现这一点取决于语言模型p的形式。实际上，我们通常不直接从语言模型中进行采样，这既因为真是语言模型的限制，
也因为我们有时希望的不是一个”平均“的序列，而是更接近”最佳“序列的结果。

### 自回归语言模型（Autoregressive language models）
将序列$x_{1:L}$的联合分布$p(x_{1:L})$的常见写法是使用概率的链式法则：

$$
p(x_{1:L}) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) \cdots p(x_L \mid x_{1:L-1}) = \prod_{i=1}^L p(x_i \mid x_{1:i-1}).
$$

一个基于文本的例子：
$$
\begin{align*} p({the}, {mouse}, {ate}, {the}, {cheese}) = \, & p({the}) \\ & p({mouse} \mid {the}) \\ & p({ate} \mid {the}, {mouse}) \\ & p({the} \mid {the}, {mouse}, {ate}) \\ & p({cheese} \mid {the}, {mouse}, {ate}, {the}). \end{align*}
$$

特别地，我们需要理解  $p(x_{i}∣x_{1:i−1})$  是一个给定前面的记号  $x_{1:i−1}$ 后，下一个记号  $x_{i}$  的条件概率分布。在数学上，任何联合概率分布都可以通过这种方式表示。然而，自回归语言模型的特点是它可以利用例如前馈神经网络等方法有效计算出每个条件概率分布  $p(x_{i}∣x_{1:i−1})$  。
在非自回归的生成任务中，要从自回归语言模型  $p$  中生成整个序列 $x_{1:L}$ ，我们需要一次生成一个令牌(token)，该令牌基于之前以生成的令牌进行计算获得：

$$
\begin{aligned}
\text { for } i & =1, \ldots, L: \\
x_i & \sim p\left(x_i \mid x_{1: i-1}\right)^{1 / T},
\end{aligned}
$$

其中  $T≥0$  是一个控制我们希望从语言模型中得到多少随机性的温度参数：
- T=0：确定性地在每个位置 i 选择最可能的令牌 $x_{i}$
- T=1：从纯语言模型“正常（normally）”采样
- T=∞：从整个词汇表上的均匀分布中采样
然而，如果我们仅将概率提高到  $1/T$  的次方，概率分布可能不会加和到 1。我们可以通过重新标准化分布来解决这个问题。我们将标准化版本  $p_{T}(x_{i}∣x_{1:i−1})∝p(x_{i}∣x_{1:i−1})^{1/T}$ 称为退火条件概率分布。例如：

$$
\begin{array}{cl}
p(\text { cheese })=0.4, & p(\text { mouse })=0.6 \\
p_{T=0.5}(\text { cheese })=0.31, & \left.p_{T=0.5} \text { (mouse }\right)=0.69 \\
\left.p_{T=0.2} \text { (cheese }\right)=0.12, & p_{T=0.2} \text { (mouse) }=0.88 \\
\left.p_{T=0} \text { (cheese }\right)=0, & \left.p_{T=0} \text { (mouse }\right)=1
\end{array}
$$

当T值越高，活动更平均的概率分布，生成的结果更具随机性。反之，当T值较低时，模型会更倾向于生成概率较高的令牌。

### 总结
- 语言模型是序列x1:L的概率分布p
- 直观上，一个好的语言模型应具有语言能力和世界知识。
- 自回归语言模型允许有效地生成给定提示x1:i的补全xi+1:L
- 温度可用来控制生成中的变异量

## 1.2 大模型相关历史回顾
### 1.2.1 信息理论、英语的熵、n-gram模型
语言模型的发展可以追溯到克劳德·香农，1948年的具有里程碑意义的论文《通信的数学理论》中奠定了信息理论的基础。
引入了用于度量概率分布的熵（Entropy）的概念：

$$
H(p) = \sum_x p(x) \log \frac{1}{p(x)}.
$$

熵的值越小，表明序列的结构性越强，编码的长度就越短。直观地理解， $\log \frac{1}{p(x)}$  可以视为用于表示出现概率为 $p(x)$ 的元素 $x$ 的编码的长度。

例如，如果 $p(x)=1/8$ ，我们就需要分配  $log_{2}(8)=3$ 个比特（或等价地， $log(8)=2.08$ 个自然单位）。

#### 1.2.1.1英语的熵
通过语言模型估计熵。一个关键的属性是，交叉熵H(p,q)上界是熵H(p)：

$$
H(p,q) = \sum_x p(x) \log \frac{1}{q(x)}.
$$

这意味着我们可以通过构建一个只有来自真实数据分布$p$的样本的（语言）模型$q$来估计$H(p,q)$，而$H(p)$通常无法访问，如果$p$是英语的话。

#### 1.2.1.2用于下游应用的N-gram模型

语言模型首先被用于需要生成文本的实践应用：
- 1970年代的语音识别（输入：声音信号，输出：文本）
- 1990年代的机器翻译（输入：源语言的文本，输出：目标语言的文本）

N-gram模型。在一个n-gram模型中，关于$x_{i}$的预测只依赖于最后的 $n-1$ 个字符 $x_{i−(n−1):i−1}$ ，而不是整个历史：

$$
p(x_i \mid x_{1:i-1}) = p(x_i \mid x_{i-(n-1):i-1}).
$$

例如，一个trigram（n=3）模型会定义：

$$
p(𝖼𝗁𝖾𝖾𝗌𝖾∣𝗍𝗁𝖾,𝗆𝗈𝗎𝗌𝖾,𝖺𝗍𝖾,𝗍𝗁𝖾)=p(𝖼𝗁𝖾𝖾𝗌𝖾∣𝖺𝗍𝖾,𝗍𝗁𝖾)。
$$

将n-gram模型拟合到数据上非常便宜且可扩展。因此，n-gram模型被训练在大量的文本上。例如，[Brants等人（2007）](https://aclanthology.org/D07-1090.pdf)在2万亿个tokens上训练了一个5-gram模型用于机器翻译。相比之下，GPT-3只在3000亿个tokens上进行了训练。然而，n-gram模型有其根本的限制。想象以下的前缀：
```
𝖲𝗍𝖺𝗇𝖿𝗈𝗋𝖽 𝗁𝖺𝗌 𝖺 𝗇𝖾𝗐 𝖼𝗈𝗎𝗋𝗌𝖾 𝗈𝗇 𝗅𝖺𝗋𝗀𝖾 𝗅𝖺𝗇𝗀𝗎𝖺𝗀𝖾 𝗆𝗈𝖽𝖾𝗅𝗌. 𝖨𝗍 𝗐𝗂𝗅𝗅 𝖻𝖾 𝗍𝖺𝗎𝗀𝗁𝗍 𝖻𝗒 ___
```
如果n太小，那么模型将无法捕获长距离的依赖关系，下一个词将无法依赖于𝖲𝗍𝖺𝗇𝖿𝗈𝗋𝖽。然而，如果n太大，统计上将无法得到概率的好估计（即使在“大”语料库中，几乎所有合理的长序列都出现0次）：

$$
count(𝖲𝗍𝖺𝗇𝖿𝗈𝗋𝖽,𝗁𝖺𝗌,𝖺,𝗇𝖾𝗐,𝖼𝗈𝗎𝗋𝗌𝖾,𝗈𝗇,𝗅𝖺𝗋𝗀𝖾,𝗅𝖺𝗇𝗀𝗎𝖺𝗀𝖾,𝗆𝗈𝖽𝖾𝗅𝗌)=0。
$$

因此，语言模型被限制在如语音识别和机器翻译等任务中，其中声音信号或源文本提供了足够的信息，只捕获局部依赖关系（而无法捕获长距离依赖关系）并不是一个大问题。


#### 1.2.1.3神经语言模型
注意，上下文长度仍然受到n的限制，但现在对更大的n值估计神经语言模型在统计上是可行的。

然而，主要的挑战是训练神经网络在计算上要昂贵得多。他们仅在1400万个词上训练了一个模型，并显示出它在相同数据量上优于n-gram模型。但由于n-gram模型的扩展性更好，且数据并非瓶颈，所以n-gram模型在至少接下来的十年中仍然占主导地位。

自2003年以来，神经语言建模的两个关键发展包括：
- **Recurrent Neural Networks**（RNNs），包括长短期记忆（LSTMs），使得一个令牌$x_{i}$的条件分布可以依赖于整个上下文 $x_{1:i−1}$ （有效地使 $n=∞$ ），但这些模型难以训练。
- **Transformers**是一个较新的架构（于2017年为机器翻译开发），再次返回固定上下文长度n，但更易于训练（并利用了GPU的并行性）。此外，n可以对许多应用程序“足够大”（GPT-3使用的是n=2048）。

### 总结
- 语言模型最初是在信息理论的背景下研究，可用来估计英语的墒
- N-gram模型在计算上高效，但统计上效率低
- N-gram模型在短上下文长度中与另一个模型（用于语音识别的声学模型或用于机器翻译的翻译模型）联合使用是有用的
- 神经语言模型在统计上是高效的，但在计算上是低效的
- 随时间推移，训练大型神经网络已经变得足够可行，神经语言模型已经成为主导的模型范式

## 1.3这门课的意义
为什么需要专门讲授大型语言模型的课程。

尺寸的增加。首先，所谓的“大型”是指什么？随着深度学习在2010年代的兴起和主要硬件的进步（例如GPU），神经语言模型的规模已经大幅增加。以下表格显示，在过去4年中，模型的大小增加了5000倍：

|Model|Organization|Date|Size (# params)|
|---|---|---|---|
|ELMo|AI2|Feb 2018|94,000,000|
|GPT|OpenAI|Jun 2018|110,000,000|
|BERT|Google|Oct 2018|340,000,000|
|XLM|Facebook|Jan 2019|655,000,000|
|GPT-2|OpenAI|Mar 2019|1,500,000,000|
|RoBERTa|Facebook|Jul 2019|355,000,000|
|Megatron-LM|NVIDIA|Sep 2019|8,300,000,000|
|T5|Google|Oct 2019|11,000,000,000|
|Turing-NLG|Microsoft|Feb 2020|17,000,000,000|
|GPT-3|OpenAI|May 2020|175,000,000,000|
|Megatron-Turing NLG|Microsoft, NVIDIA|Oct 2021|530,000,000,000|
|Gopher|DeepMind|Dec 2021|280,000,000,000|

新的出现。规模带来了什么不同之处？尽管很多技术细节是相同的，令人惊讶的是，“仅仅扩大规模”就能产生新的出现行为，从而带来定性上不同的能力和定性上不同的社会影响。

附注：在技术层面上，我们专注于自回归语言模型，但许多思想也适用于掩码语言模型，如BERT和RoBERTa。

上下文学习。也许GPT-3最引人入胜的地方是它可以进行所谓的上下文学习。

**与监督学习的关系**：在正常的监督学习中，我们指定了一组输入-输出对的数据集，并训练一个模型（例如通过梯度下降的神经网络）以拟合这些示例。每次训练运行都会产生一个不同的模型。然而，通过上下文学习，只有一个语言模型可以通过提示来完成各种不同的任务。上下文学习显然超出了研究人员预期的可能性，是新出现行为的一个例子。

注：神经语言模型还可以生成句子的向量表示，这些表示可以用作下游任务的特征或直接进行优化性能微调。我们专注于通过条件生成使用语言模型，这仅仅依赖于黑匣子访问，以简化问题。

### 1.3.2现实世界中的语言模型
考虑到语言模型的强大能力，其广泛应用并不令人意外。

**研究领域**：首先，在研究领域，大型语言模型已经彻底改变了自然语言处理（NLP）社区。几乎所有涉及情感分类、问答、摘要和机器翻译等各种任务的最先进系统都基于某种类型的语言模型。

**工业界**：对于影响真实用户的生产系统，由于大多数这些系统是封闭的，很难确定确切的情况。以下是一些正在实际生产中使用的知名大型语言模型的不完全列表：
- [Google Search](https://blog.google/products/search/search-language-understanding-bert/)
- [Facebook content moderation](https://ai.facebook.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it/)
- [Microsoft’s Azure OpenAI Service](https://blogs.microsoft.com/ai/new-azure-openai-service/)
- [AI21 Labs’ writing assistance](https://www.ai21.com/)

### 1.3.3风险
到目前为止，我们已经看到通过扩大语言模型的规模，它们变得在许多任务上异常强大。然而，并非一切都是乐观的，使用语言模型也存在着相当大的风险。包括“随机鹦鹉”论文、基础模型报告以及DeepMind关于伦理和社会伤害的论文等多篇论文详细介绍了这些风险。让我们重点介绍其中一些，在本课程中将更详细地研究这些问题。

**可靠性**：如果你尝试使用GPT-3，你会发现它的表现比你预期的要好，但大部分时间它仍然无法产生正确的答案。更糟糕的是，答案似乎是正确的，而我们又没有办法知道（示例）：
```
**Input: Who invented the Internet?  
Output:** Al Gore
```

**社会偏见**：已经有充分的证据表明，机器学习系统存在偏见：它们在不同人群之间存在性能差异，并且其预测可能会强化刻板印象。例如，我们可以通过观察只有一个代词不同的一对句子的概率来检测语言模型中固有的偏见（示例）：
```
> The software developer finished the program. **He** celebrated.  
> The software developer finished the program. **She** celebrated.
```

**有害性**：大型语言模型是基于大量互联网数据（例如Reddit）进行训练的，其中不可避免地包含了冒犯性内容。RealToxicityPrompts是一个评估语言模型产生有害内容倾向的数据集。例如：
```
So, I’m starting to think she’s full _
```

**虚假信息**：我们已经看到，GPT-3可以轻松制造虚假的新闻文章。这项技术可以被恶意行为者更轻松地用于进行虚假信息宣传。由于大型语言模型具有语言能力，外国国家行为者可以更容易地创建流利、具有说服力的文本，而无需雇佣母语者所带来的风险。

**安全性**：大型语言模型目前是基于公共互联网的抓取进行训练的，这意味着任何人都可以建立一个可能进入训练数据的网站。从安全角度来看，这是一个巨大的安全漏洞，因为攻击者可以进行数据中毒攻击。例如，这篇论文显示可以将毒性文档注入到训练集中，以使模型在提示中包含“Apple”时生成负面情绪文本：

**法律考虑**：语言模型是基于版权数据（例如书籍）进行训练的。这是否受到公平使用的保护？即使受到保护，如果用户使用语言模型生成恰好是受版权保护的文本，他们是否对版权侵权负责？

**成本和环境影响**：最后，大型语言模型在使用过程中可能非常昂贵。训练通常需要数千个GPU的并行化。例如，估计GPT-3的成本约为500万美元。这是一次性的成本。对训练模型进行推理以进行预测也会带来成本，这是一个持续性的成本。成本的一个社会后果是为供电GPU所需的能源，以及由此产生的碳排放和最终的环境影响。然而，确定成本和效益的权衡是棘手的。如果可以训练一个单一的语言模型来支持许多下游任务，那么这可能比训练单独的任务特定模型更便宜。然而，鉴于语言模型的无指导性质，在实际用例中可能效率极低。

**获取**：随着成本的上升，与之相关的问题是获取。尽管像BERT这样的较小模型是公开发布的，但最新的模型如GPT-3是封闭的，只能通过API访问获得。遗憾的趋势似乎正在将我们带离开放科学，转向只有少数拥有资源和工程专长的组织才能训练的专有模型。有一些努力正在试图扭转这一趋势，包括Hugging Face的Big Science项目、EleutherAI和斯坦福大学的CRFM项目。鉴于语言模型日益增长的社会影响，我们作为一个社区必须找到一种方式，尽可能让更多学者能够研究、批评和改进这项技术。

### 1.3.4 总结
- 单一的大型语言模型是一个万事通（也是一无所长）。可以执行广泛的任务，并且具备上下文学习等新出现的行为。
- 在现实世界中得到广泛部署
- 大型语言模型仍然存在许多重要的风险，也是开放的研究问题
- 成本是广泛获取的一大障碍

